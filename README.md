# Data Lake with Spark
## Daniel Chang

## Overview
A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This helps their analytics team to continue finding insights in what songs their users are listening to.

## Project Datasets
- **Song data('s3://udacity-dend/song_data')**: The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.<br>Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset listed down below.  
>**s3://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json**<br>
>**s3://udacity-dend/song_data/A/A/B/TRAABJL12903CDCF1A.json**

- **Log data('s3://udacity-dend/log_data')**: The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. <br>The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
>**s3://udacity-dend/log_data/2018/11/2018-11-12-events.json**<br>
>**s3://udacity-dend/log_data/2018/11/2018-11-13-events.json**

## Project Templates
1. `etl.py`: retrieves the song and log data in the s3 bucket, transforms the data into fact and dimensional tables then loads the table data back into s3 as parquet files.
2. `dwh.cfg`: contains configuration for Redshift database. Please edit accordingly if you plan to use.
3. `README.md`: provide discussion on process and decisions for this ETL pipeline.
4. `Create tables.ipynb`: This is a testing notebook that retrieves the song and log data in s3  bucket and transform the data into fact and dimensional tables before putting them into the `etl.py`.
5. `data/`: This folder consists of a subset of the full dataset.

## Data Schema
**Fact Table**  
1. songplays - records in log data associated with song plays i.e. records with page NextSong
    * songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent  
**Dimension Tables**
2. users - users in the app  
   * user_id, first_name, last_name, gender, level  
3. songs - songs in music database    
   * song_id, title, artist_id, year, duration  
4. artists - artists in music database   
   * artist_id, name, location, latitude, longitude  
5. time - timestamps of records in songplays broken down into specific units  
   * start_time, hour, day, week, month, year, weekday

## How to Run
1. Set appropriate AWS IAM Credentials(`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`) in dl.cfg.
2. Create an S3 bucket and replace the `output_data` variable in the `main()` function with `s3a://<bucket name>/`.Please read thru the comments in `etl.py`carefully. Some additional changes might be needed. For example, this project has both a subset of the song data and the full data in its code(make sure to choose one and comment out the other one). **Please note that the `etl.py` currently only runs subsets of the full dataset.**
3. Run ETL pipeline by typing in python etl.py your terminal.

## License and Acknowledgement
I would like to give special thank to [Udacity](www.udacity.com) for giving me the change to work on this project. The dataset used in this project can be found at [Million Song Dataset](millionsongdataset.com).
